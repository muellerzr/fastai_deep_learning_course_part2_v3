{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Precision Training\n",
    "\n",
    "#### Last Time\n",
    "We've [just finished](http://nbviewer.jupyter.org/github/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/10b_mixup_label_smoothing_my_reimplementation.ipynb?flush_cache=true) implementing [Mixup](https://arxiv.org/abs/1710.09412) and [Label Smoothing](https://arxiv.org/abs/1512.00567). Our Mixup implementation included a few optimizations to the original paper's approach that resulted in a sped up training time:\n",
    "* Not loading in two batches at once, but shuffling one batch's images and mixing up half of them with the other half.\n",
    "* Using a different ratio, $t$, to mixup each image pair in a batch.\n",
    "\n",
    "Encouragingly, I found that training a model using mixup resulted in better performance than [when I trained](https://nbviewer.jupyter.org/github/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/Diving_into_DALI.ipynb?flush_cache=true#Bonus:-horizontal-flip-transform-experiment) the same the same model using a DALI pipeline with four more traditional image augmentations. Unfortunately, adding label smoothing to my mixup model's loss function did not improve performance.\n",
    "\n",
    "#### FP16 Training\n",
    "Today we're gonna spend some time understand what, exactly, mixed-precision training is, and why using it when training models can be helpful. This topic is personally relevent to me, because I will shortly be building my own deep learning work station from scratch, and NVIDIA's latest GPUs, such as the [RTX 2080 Ti](https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/) support mixed-precision training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "As always, we're going to train our models on the [Imagenette](https://github.com/fastai/imagenette) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exports.nb_10b import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(imagenette_url)\n",
    "tfms = [MakeRGB(), ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
    "bs=64\n",
    "\n",
    "data = (ImageList.from_files(path, tfms=tfms)\n",
    "        .to_split(partial(grandparent_splitter, valid_name='val'))\n",
    "        .to_label(parent_labeler, y_processor=CategoryProcessor())\n",
    "        .to_databunch(bs, channels_in=3, channels_out=10, num_workers=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What *is* mixed-precision training?\n",
    "Up until now, all calculations in neural networks have typically been done using 32-bit single precision floats. One problem that practitioners universally face is running out of GPU memory while training. This happens when networks have many layers, and or, a large batch size is being used. Since 16-bit floats take up about half the memory that 32-bit floats require, some practitioners have lately begun to try to sidestep GPU memory bottlenecks by training networks that use 16-bit floats in their calculations.\n",
    "\n",
    "Moreover, as mentioned above, NVIDIA's latest GPUs have been optimized to do more operations at the same time when 16-bit floats are used, which has the nice side-effect of decreasing training time. Unfortunately, just how much of a speed-up you get depends on which GPU you happen to be using. (Volta GPUs are said to offer the largest speed increase.)\n",
    "\n",
    "### We can't *only* use half-precision training\n",
    "There's one slight problem: if we only use half-precision training, we'll find that our networks don't converge to as good an accuracy as would have been the case had we used full-precision 32-bit float training. To understand why this could happen, let's disect what a 16-bit float actually is:\n",
    "\n",
    "![half float](images/half.png)\n",
    "\n",
    "FP16 offers a smaller range of possible values, from 2e-14 to 2e15, than FP32, which give a range of 2e-126 to 2e127. Another limitation of FP16 is that it supports a larger minimum *offset* than FP32. Between numbers 1 and 2, FP16 only represents the numbers 1, 1+2e-10, 1+2*2e-10, 1+3*2e-10, ... and so on and so forth. The implication of this larger offset is that in the world of FP16, 1 + 0.0001 is only seen as 1. This imprecision could lead to the following three problems during model training:\n",
    "\n",
    "#### 3 Problems with Half-precision training\n",
    "1. Weight gradients, once multiplied by the learning rate, are often smaller than the FP16 offset, which means that there's a significant chance that the model's layer weights won't actually get updated and change after any given training iteration.\n",
    "2. Similarly, weight gradients themselves can be so small that FP16 thinks they are zero. This will also cause layer weights, particularly at the earliest layers, to not change during training.\n",
    "3. On the other end of the spectrum, there's a greater chance of gradients exploding when FP16 is used. Since the largest number that FP16 can represent is several orders of magnitude smaller than that which can be represented by FP32, there's a much greater chance that gradients will hit infinity and be seen as NaN. \n",
    "\n",
    "### The right approach: use a *mix* of FP16 and FP32\n",
    "Thankfully, there's a workaround that will let us get the memory saving benefits of FP16 training, while making it possible to avoid the three bugaboos described just above: we perform some operations in FP16, and the rest we will do in FP32!\n",
    "\n",
    "#### Avoiding Problem 1\n",
    "We can do the forward pass in FP16, because this will give us some extra speed and having relatively less precision during the forward pass doesn't bring any other bad consequences. In order to avoid the first problem detailed above (weights *don't* actually get updated during the \"weight update\"), we perform the weight update operation, `w = w - lr * grad`, *using FP32*.\n",
    "\n",
    "#### Avoiding Probem 2\n",
    "In order to avoid problem two, where FP16 takes gradients that actually are larger than 0 in magnitude but sees them as 0 nonetheless (known as *gradient underflow*), the trick is to perform something called *gradient scaling*. All this entails is multiplying by a scale factor, such as 512, the loss value we get from the loss function. Doing this allows us to also perform the backward pass in FP16, without having to worry that gradients (especially those backpropagating through the network's the earliest layers) will become 0. Of course, when we're back in FP32 and about to perform the weight update calculation, we need to divide the gradient by the scaling factor before we perform the weight update.\n",
    "\n",
    "#### Avoiding Problem 3\n",
    "To lessen the chance of having exploding gradients due to FP16 prematurely seeing large gradients as infinite, or NaN, there are two things we can do:\n",
    "* Convert the output of model's final layer from FP16 to FP32 and compute the loss using FP32. \n",
    "* Leave all batchnorm layer weights in FP32.\n",
    "\n",
    "Making our loss value more precise means that we can scale by our scale factor, such as 512, and not have to worry that the product will be seen as NaN by FP16. \n",
    "\n",
    "Batchnorm layers don't have many weights to begin with, so we can afford to store their weights in FP32 without worrying about using up significantly more GPU memory than we would if we stored those weights in FP16.\n",
    "\n",
    "#### The Canonical Mixed-Precision Training Loop\n",
    "\n",
    "![Mixed precision training](images/Mixed_precision.jpeg)\n",
    "\n",
    "Here are the steps we have in a mixed-precision training loop:\n",
    "1. With FP16: Run the forward pass.\n",
    "2. With FP32: Convert the final layer outputs from FP16 to FP32, and compute the loss.\n",
    "3. With FP32: Multiply the loss by the gradient scale factor, such as 512.\n",
    "4. With FP16: Backpropagate the gradients.\n",
    "5. With FP32: Copy the FP16 gradients to FP32.\n",
    "6. With FP32: Divide gradients by the scale factor applied in step 3., and calculate and apply layer weight updates.\n",
    "7. With FP16: Copy the newly updated gradients from FP32 back to FP16, in preparation to run the forward pass of the next loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Mixed-Precision Training Loop using Callbacks\n",
    "\n",
    "### Helper Functions\n",
    "Before we implement the our `MixedPrecision` callback class, we need to write some helper functions that will help us to do things like convert a model to FP16, or create a copy of model parameters.\n",
    "\n",
    "#### Helper Prep: NVIDIA's Apex Library\n",
    "[Apex](https://github.com/NVIDIA/apex) is a PyTorch extension created by NVIDIA that enables mixed-precision training. We'll use some of its useful methods inside our helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import apex.fp16_utils as fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper 1: Converting the model to FP16\n",
    "Our helper function converts all layers of our model from FP32 to FP16 - except for the batchnorm layers, which are always left in FP32. The most straightforward way to accomplish this is:\n",
    "1. Convert all layers to FP16 in one fell swoop.\n",
    "2. Loop back over all layers and convert any batchnorm layers back to FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_layers = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_to_float(model):\n",
    "    if isinstance(model, bn_layers): model.float()\n",
    "    for child in model.children(): bn_to_float(child)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_half(model):\n",
    "    model = model.half()\n",
    "    return bn_to_float(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test to ensure everything works as we hope:\n",
    "\n",
    "First use our helper function to convert the model minus batchnorm layers to FP16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()\n",
    "test_model = model_to_half(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a function to verify that the weights of each of our test model's layers are as we'd expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weights(test_model):\n",
    "    for i, t in enumerate([torch.float16, torch.float32, torch.float16]):\n",
    "        assert test_model[i].weight.dtype == t\n",
    "        assert test_model[i].bias.dtype   == t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_weights(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apex library also has a function called `convert_network` that converts all model layers, save batchnorm, to FP16. Conveniently, we can use to to convert a model either *to* FP16, or *from* FP16 back to FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()\n",
    "test_model = fp16.convert_network(test_model, torch.float16)\n",
    "check_weights(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper 2: Creating a FP32 master copy of parameter weights\n",
    "While our layer weights are in FP16, we will need to perform each weight step in the optimizer using FP32. We'll thus need a way to convert these FP16 parameters to FP32 right before we perform each optimizer step.\n",
    "\n",
    "We'll also include the option to concetenate all the model paramters into one single flat tensor because this can make things run slightly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "def get_master(model, flat_master=False):\n",
    "    model_params = [param for param in model.parameters() if param.requires_grad]\n",
    "    if flat_master:\n",
    "        master_params = parameters_to_vector([param.data.float() for param in model_params])\n",
    "        master_params = torch.nn.Parameter(master_params, requires_grad=True)\n",
    "        if master_params.grad is None: master_params.grad = master_params.new(*master_params.size())\n",
    "        return model_params, [master_params]\n",
    "    else:\n",
    "        master_params = [param.clone().float().detach() for param in model_params]\n",
    "        for param in master_params: param.requires_grad_(True)\n",
    "        return model_params, master_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_params, test_master_params = get_master(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to ensure that the values of a master list of FP32 params are sufficiently close to their FP16 counterparts, and that the same params in each list are trainable (requires_grad = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_lists(param_list1, param_list2):\n",
    "    assert len(param_list1) == len(param_list2)\n",
    "    for (param1, param2) in zip(param_list1, param_list2):\n",
    "        assert param1.requires_grad == param2.requires_grad\n",
    "        assert torch.allclose(param1.data.float(), param2.data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_lists(test_model_params, test_master_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apex also has a function that we can use to get a master list of FP32 params. It's called `prep_param_lists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_params_apex, test_master_params_apex = fp16.prep_param_lists(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_lists(test_model_params_apex, test_master_params_apex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also verify that our home-grown `get_master` method and Apex's `prep_param_lists` method indeed produce the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_lists(test_model_params, test_model_params_apex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_lists(test_master_params, test_master_params_apex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can't use a flat master list when we have batchnorm layers that will always stay in FP32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_params_flat, test_master_params_flat = get_master(test_model, flat_master=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-675b5e1405e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msame_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_params_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_master_params_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-0fffd2d058d2>\u001b[0m in \u001b[0;36msame_lists\u001b[0;34m(param_list1, param_list2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msame_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mparam1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "same_lists(test_model_params_flat, test_master_params_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in prep_param_lists:  model may contain a mixture of parameters of different types.  Use flat_master=False, or use F16_Optimizer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Half but got scalar type Float for sequence element 2 in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-83999d99ccf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model_params_apex_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_master_params_apex_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_param_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_master\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/apex/fp16_utils/fp16util.py\u001b[0m in \u001b[0;36mprep_param_lists\u001b[0;34m(model, flat_master)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# flatten_dense_tensors returns a contiguous flat array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# http://pytorch.org/docs/master/_modules/torch/_utils.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmaster_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten_dense_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             print(\"Error in prep_param_lists:  model may contain a mixture of parameters \"\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_flatten_dense_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Half but got scalar type Float for sequence element 2 in sequence argument at position #1 'tensors'"
     ]
    }
   ],
   "source": [
    "test_model_params_apex_flat, test_master_params_apex_flat = fp16.prep_param_lists(test_model, flat_master=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna need to tweak our implementation because in practice we definitely won't all of our model's parameters to be in the same parameter group, which is what our current implementation of `get_master` requires. We want to support multiple parameter groups in the evnt that we want to:\n",
    "* Perform transfer learning, which requires us to freeze earlier layers of our network.\n",
    "* Apply discriminative learning rates/schedules to different layer groups.\n",
    "* Not apply weight decay to batchnorm layer or bias terms.\n",
    "\n",
    "`get_master` should thus be able to split the parameters of an optimizer (*not* those of a model) according to their respective parameter groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_master(optimizer, flat_master=False):\n",
    "    model_params = [[param for param in param_group if param.requires_grad] for param_group in optimizer.param_groups]\n",
    "    if flat_master:\n",
    "        master_params = []\n",
    "        for param_group in model_params:\n",
    "            master_param_group = parameters_to_vector([param.data.float() for param in param_group])\n",
    "            master_param_group = torch.nn.Parameter(master_param_group, requires_grad=True)\n",
    "            if master_param_group.grad is None: \n",
    "                master_param_group = master_param_group.new(*master_param_group.size())\n",
    "            master_params.append(master_param_group)\n",
    "    else:\n",
    "        master_params = [[param.clone().float().detach() for param in param_group] for param_group in model_params]\n",
    "        for param_group in master_params:\n",
    "            for param in param_group: param.requires_grad_(True)\n",
    "    return model_params, master_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper 3: Creating a FP32 master copy of gradients\n",
    "In order to perform the optimizer step, we need *both* the model paramater weights and gradients to be in FP32. The `get_master` function we implemented just above takes care of copying parameter weights to FP32. Our 3rd helper function will do the same, but for the gradients that would have been computed during backpropagation in FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_master_grads(model_params, master_params, flat_master:bool=False) -> None:\n",
    "    if flat_master:\n",
    "        if master_params[0].grad is None:\n",
    "            master_params[0].data.new(*master_params[0].data.size())\n",
    "        master_params[0].grad.data.copy_(parameters_to_vector([param.grad.data.float() for param in model_params]))\n",
    "    else:\n",
    "        for model_param, master_param in zip(model_params, master_params):\n",
    "            if model_param.grad is not None:\n",
    "                if master_param.grad is None: \n",
    "                    master_param.grad = master_param.data.new(*master_param.data.size())\n",
    "                master_param.grad.data.copy_(model_param.grad.data)\n",
    "            else: master_param.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. First we'll simulate both a forward and backward pass that are performed in FP16, for a simple model with *no* parameter groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 10).half().cuda()\n",
    "y = test_model(x)\n",
    "loss = F.cross_entropy(y, torch.randint(0, 2, (20,)).cuda())\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1296, -0.5391],\n",
       "        [ 0.2874, -0.3467],\n",
       "        [-0.0869, -0.0266],\n",
       "        [ 0.2837, -0.1782],\n",
       "        [ 0.0437,  0.0309],\n",
       "        [-0.5664,  0.0712],\n",
       "        [-0.1545, -0.1049],\n",
       "        [ 0.2072,  0.0307],\n",
       "        [ 0.0306, -0.0629],\n",
       "        [ 0.1960, -0.3071],\n",
       "        [ 0.3550, -0.1348],\n",
       "        [-0.6455,  0.0584],\n",
       "        [-0.3154, -0.2942],\n",
       "        [ 0.5537, -0.2245],\n",
       "        [ 0.2050, -0.3508],\n",
       "        [-0.4912,  0.1227],\n",
       "        [ 0.1016, -0.2094],\n",
       "        [-0.5332, -0.0906],\n",
       "        [-0.3108, -0.0751],\n",
       "        [ 0.4629,  0.2021]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7646, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_params = [param for param in test_model.parameters() if param.requires_grad]\n",
    "test_master_params = [param.clone().float().detach() for param in test_model_params]\n",
    "test_master_params = [torch.nn.Parameter(param, requires_grad=True) for param in test_master_params]\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 6.0028e-02,  1.1719e-01,  1.6040e-01,  8.0688e-02,  2.0630e-01,\n",
       "           6.4087e-02, -1.9250e-01, -3.1055e-01, -2.7637e-01,  1.3049e-01],\n",
       "         [ 4.4952e-02,  1.7444e-01,  2.8638e-01,  3.7323e-02,  2.6978e-01,\n",
       "          -1.5881e-01,  3.0786e-01, -6.8787e-02, -1.6296e-01,  2.7832e-01],\n",
       "         [-2.8198e-01,  2.8000e-02, -1.6211e-01,  2.4426e-01,  9.9915e-02,\n",
       "           2.2021e-01, -2.6001e-01,  2.4036e-01, -6.3171e-02, -3.0981e-01],\n",
       "         [-2.9816e-02,  1.9336e-01,  2.2766e-01,  2.9297e-01, -8.6426e-02,\n",
       "           1.0162e-01,  5.1666e-02,  2.5610e-01, -5.9326e-02,  1.0303e-01],\n",
       "         [ 2.4475e-01, -6.2439e-02,  1.3171e-01,  1.0767e-01,  3.3417e-02,\n",
       "          -2.8345e-01,  1.7432e-01,  3.0127e-01,  1.6040e-01, -8.7219e-02],\n",
       "         [-1.1328e-01,  2.5171e-01,  1.6284e-01, -7.9407e-02, -2.9663e-01,\n",
       "          -7.6050e-02,  2.2705e-01, -2.9144e-03, -3.1396e-01, -2.0520e-01],\n",
       "         [ 2.8882e-01, -8.4167e-02,  1.1261e-01, -2.9785e-01,  9.6680e-02,\n",
       "           2.6685e-01, -1.9238e-01, -2.7490e-01,  1.2549e-01,  2.3999e-01],\n",
       "         [ 3.2387e-03,  8.5083e-02,  1.2537e-01,  2.5586e-01,  2.1045e-01,\n",
       "          -3.0518e-01,  9.8633e-02, -3.0176e-01, -7.4707e-02,  1.0114e-01],\n",
       "         [-4.6265e-02,  1.3586e-01,  1.7273e-01, -2.8137e-02,  1.6284e-01,\n",
       "          -4.6082e-02, -4.9210e-03, -1.9897e-01, -1.3513e-01,  8.7524e-02],\n",
       "         [ 9.4604e-02,  9.9670e-02,  1.2939e-01, -6.0242e-02, -1.6516e-01,\n",
       "          -3.0688e-01,  1.6357e-01, -2.7026e-01, -3.1421e-01,  1.0016e-01],\n",
       "         [ 7.2998e-02, -3.0957e-01,  1.6711e-01,  2.7002e-01,  1.0480e-01,\n",
       "           3.0981e-01,  4.1382e-02,  5.2109e-03, -1.1459e-02, -2.7319e-01],\n",
       "         [-1.3721e-01, -1.4441e-01,  1.4722e-01, -2.7466e-01, -1.5063e-01,\n",
       "          -5.6488e-02, -1.1267e-01, -8.8577e-03, -5.6381e-03, -1.0944e-01],\n",
       "         [ 2.8662e-01,  7.5623e-02,  1.6333e-01, -1.9800e-01, -6.5369e-02,\n",
       "          -1.8750e-01,  1.2878e-01, -2.9150e-01,  5.1483e-02,  1.1316e-01],\n",
       "         [ 2.5098e-01,  2.2119e-01,  5.1025e-02,  2.9907e-01,  2.5708e-01,\n",
       "           2.8992e-02,  2.8687e-01,  5.1697e-02,  4.7546e-02, -1.0327e-01],\n",
       "         [-2.6489e-01, -1.8262e-01, -3.9101e-04, -2.2510e-01, -2.1655e-01,\n",
       "          -8.6975e-03,  2.2253e-01, -1.1063e-02,  1.4783e-01,  5.0446e-02],\n",
       "         [-4.9713e-02,  2.9028e-01,  2.2827e-01, -7.5500e-02,  3.0029e-01,\n",
       "           3.1403e-02, -3.5309e-02,  2.8394e-01, -2.6392e-01,  5.6549e-02],\n",
       "         [ 1.9727e-01, -9.9792e-02,  1.5173e-01, -2.5830e-01, -3.0835e-01,\n",
       "           1.7517e-01, -2.4829e-01,  7.0984e-02,  1.9617e-01, -2.4986e-04],\n",
       "         [ 1.1322e-01,  3.0090e-02, -5.6732e-02, -1.9446e-01, -1.3696e-01,\n",
       "           3.1177e-01, -2.4872e-02, -7.2754e-02, -1.6760e-01,  2.0654e-01],\n",
       "         [ 6.9702e-02,  1.4209e-01,  2.9404e-02,  2.1802e-01,  1.8811e-01,\n",
       "           3.3386e-02,  1.2378e-01, -2.3584e-01,  2.1130e-01,  1.4856e-01],\n",
       "         [ 1.6614e-01,  2.6123e-02,  1.7419e-01, -1.8530e-01,  2.8857e-01,\n",
       "           7.8491e-02, -2.6733e-01, -1.8677e-01, -2.0801e-01,  1.7407e-01],\n",
       "         [-2.7283e-02, -3.0298e-01,  3.0078e-01, -3.3722e-02, -3.7476e-02,\n",
       "          -5.5206e-02,  1.6089e-01,  1.6016e-01, -1.4099e-02,  8.0078e-02],\n",
       "         [-1.1981e-01, -6.1737e-02, -7.4890e-02, -2.8296e-01,  2.3474e-01,\n",
       "          -5.9570e-02, -1.4136e-01, -1.1707e-01,  3.3844e-02, -7.4043e-03],\n",
       "         [ 6.0913e-02,  1.3086e-01, -2.8748e-02,  2.6611e-01, -2.8882e-01,\n",
       "          -2.9077e-01, -2.8638e-01, -2.6367e-01,  9.3689e-02,  1.0846e-01],\n",
       "         [ 1.2683e-01, -6.2378e-02, -2.0398e-01,  2.9834e-01,  6.4453e-02,\n",
       "           8.6243e-02,  3.7415e-02,  2.8784e-01, -1.2703e-02,  4.3915e-02],\n",
       "         [-2.2974e-01, -1.7725e-01, -1.7395e-01, -1.9104e-01, -2.2632e-01,\n",
       "           1.4282e-01,  1.7444e-01,  6.6711e-02,  2.8564e-01,  1.4392e-01],\n",
       "         [-3.0225e-01, -5.5298e-02, -7.6111e-02, -6.4880e-02, -3.5973e-03,\n",
       "          -1.2344e-02, -1.4429e-01,  7.2693e-02,  1.5820e-01, -2.8125e-01],\n",
       "         [ 3.0853e-02,  1.5186e-01, -2.3560e-01,  7.7393e-02, -2.2498e-01,\n",
       "          -2.4268e-01, -7.1045e-02,  2.0154e-01,  1.3435e-02,  1.5967e-01],\n",
       "         [ 1.3403e-01,  6.7078e-02,  2.7466e-01,  1.4734e-01, -8.2214e-02,\n",
       "          -2.7148e-01,  2.5903e-01,  9.4421e-02, -1.9910e-01,  2.1765e-01],\n",
       "         [-6.9946e-02,  1.6528e-01, -2.4451e-01,  1.4648e-01,  1.4709e-01,\n",
       "           1.1196e-03,  5.2032e-02, -1.6174e-02, -4.5135e-02,  2.5244e-01],\n",
       "         [ 4.2480e-02,  1.2854e-01,  2.7856e-01,  1.0498e-01, -2.3914e-01,\n",
       "          -1.7786e-01,  1.3879e-01,  1.5427e-02, -3.0298e-01,  1.6467e-01]],\n",
       "        device='cuda:0', dtype=torch.float16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2981, -0.1270, -0.2142, -0.0845, -0.1183, -0.1981,  0.0138,  0.0109,\n",
       "         -0.2356,  0.1774,  0.2527,  0.3120, -0.2346,  0.2241,  0.0157,  0.0607,\n",
       "         -0.2418,  0.1060,  0.0296,  0.1058, -0.1373,  0.1044, -0.0075, -0.1982,\n",
       "         -0.0453,  0.2876,  0.2817, -0.2317,  0.2098,  0.1152], device='cuda:0',\n",
       "        dtype=torch.float16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2933, 0.0328, 0.4102, 0.8385, 0.7705, 0.7688, 0.1496, 0.8441, 0.0049,\n",
       "         0.8729, 0.6137, 0.8909, 0.3187, 0.3351, 0.1173, 0.8980, 0.6859, 0.1560,\n",
       "         0.8973, 0.1757, 0.8740, 0.5098, 0.6870, 0.5428, 0.8056, 0.7795, 0.4494,\n",
       "         0.6531, 0.0140, 0.9588], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2512e-01, -1.2286e-01, -7.8247e-02, -1.5723e-01,  1.2158e-01,\n",
       "           1.0559e-01, -8.5022e-02, -6.2744e-02, -1.8635e-03,  8.6426e-02,\n",
       "           1.2781e-01,  1.5186e-01,  3.9520e-02,  3.4241e-02, -5.4657e-02,\n",
       "          -1.7664e-01,  3.1815e-03,  1.1371e-01, -4.3579e-02, -1.5039e-01,\n",
       "           1.1133e-01,  2.2202e-02, -1.7871e-01, -1.9028e-02, -1.5186e-01,\n",
       "           3.4851e-02,  1.7502e-02,  1.0657e-01, -1.4136e-01,  1.6565e-01],\n",
       "         [-3.3051e-02, -1.4612e-01,  7.5928e-02,  3.6560e-02, -7.6477e-02,\n",
       "          -8.1116e-02,  6.5430e-02, -9.4360e-02,  4.7668e-02,  3.4851e-02,\n",
       "          -6.5796e-02,  5.0323e-02, -4.2267e-02, -1.1421e-02, -1.1658e-02,\n",
       "          -8.2397e-02, -1.5308e-01, -1.2341e-01,  8.8573e-05,  4.7760e-02,\n",
       "          -2.0645e-02, -8.8684e-02,  1.3367e-01,  3.4027e-02, -8.7280e-02,\n",
       "           1.0236e-01, -1.8237e-01,  9.6130e-02,  2.6321e-03,  3.6926e-02]],\n",
       "        device='cuda:0', dtype=torch.float16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0124, -0.1214], device='cuda:0', dtype=torch.float16,\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 6.0028e-02,  1.1719e-01,  1.6040e-01,  8.0688e-02,  2.0630e-01,\n",
       "           6.4087e-02, -1.9250e-01, -3.1055e-01, -2.7637e-01,  1.3049e-01],\n",
       "         [ 4.4952e-02,  1.7444e-01,  2.8638e-01,  3.7323e-02,  2.6978e-01,\n",
       "          -1.5881e-01,  3.0786e-01, -6.8787e-02, -1.6296e-01,  2.7832e-01],\n",
       "         [-2.8198e-01,  2.8000e-02, -1.6211e-01,  2.4426e-01,  9.9915e-02,\n",
       "           2.2021e-01, -2.6001e-01,  2.4036e-01, -6.3171e-02, -3.0981e-01],\n",
       "         [-2.9816e-02,  1.9336e-01,  2.2766e-01,  2.9297e-01, -8.6426e-02,\n",
       "           1.0162e-01,  5.1666e-02,  2.5610e-01, -5.9326e-02,  1.0303e-01],\n",
       "         [ 2.4475e-01, -6.2439e-02,  1.3171e-01,  1.0767e-01,  3.3417e-02,\n",
       "          -2.8345e-01,  1.7432e-01,  3.0127e-01,  1.6040e-01, -8.7219e-02],\n",
       "         [-1.1328e-01,  2.5171e-01,  1.6284e-01, -7.9407e-02, -2.9663e-01,\n",
       "          -7.6050e-02,  2.2705e-01, -2.9144e-03, -3.1396e-01, -2.0520e-01],\n",
       "         [ 2.8882e-01, -8.4167e-02,  1.1261e-01, -2.9785e-01,  9.6680e-02,\n",
       "           2.6685e-01, -1.9238e-01, -2.7490e-01,  1.2549e-01,  2.3999e-01],\n",
       "         [ 3.2387e-03,  8.5083e-02,  1.2537e-01,  2.5586e-01,  2.1045e-01,\n",
       "          -3.0518e-01,  9.8633e-02, -3.0176e-01, -7.4707e-02,  1.0114e-01],\n",
       "         [-4.6265e-02,  1.3586e-01,  1.7273e-01, -2.8137e-02,  1.6284e-01,\n",
       "          -4.6082e-02, -4.9210e-03, -1.9897e-01, -1.3513e-01,  8.7524e-02],\n",
       "         [ 9.4604e-02,  9.9670e-02,  1.2939e-01, -6.0242e-02, -1.6516e-01,\n",
       "          -3.0688e-01,  1.6357e-01, -2.7026e-01, -3.1421e-01,  1.0016e-01],\n",
       "         [ 7.2998e-02, -3.0957e-01,  1.6711e-01,  2.7002e-01,  1.0480e-01,\n",
       "           3.0981e-01,  4.1382e-02,  5.2109e-03, -1.1459e-02, -2.7319e-01],\n",
       "         [-1.3721e-01, -1.4441e-01,  1.4722e-01, -2.7466e-01, -1.5063e-01,\n",
       "          -5.6488e-02, -1.1267e-01, -8.8577e-03, -5.6381e-03, -1.0944e-01],\n",
       "         [ 2.8662e-01,  7.5623e-02,  1.6333e-01, -1.9800e-01, -6.5369e-02,\n",
       "          -1.8750e-01,  1.2878e-01, -2.9150e-01,  5.1483e-02,  1.1316e-01],\n",
       "         [ 2.5098e-01,  2.2119e-01,  5.1025e-02,  2.9907e-01,  2.5708e-01,\n",
       "           2.8992e-02,  2.8687e-01,  5.1697e-02,  4.7546e-02, -1.0327e-01],\n",
       "         [-2.6489e-01, -1.8262e-01, -3.9101e-04, -2.2510e-01, -2.1655e-01,\n",
       "          -8.6975e-03,  2.2253e-01, -1.1063e-02,  1.4783e-01,  5.0446e-02],\n",
       "         [-4.9713e-02,  2.9028e-01,  2.2827e-01, -7.5500e-02,  3.0029e-01,\n",
       "           3.1403e-02, -3.5309e-02,  2.8394e-01, -2.6392e-01,  5.6549e-02],\n",
       "         [ 1.9727e-01, -9.9792e-02,  1.5173e-01, -2.5830e-01, -3.0835e-01,\n",
       "           1.7517e-01, -2.4829e-01,  7.0984e-02,  1.9617e-01, -2.4986e-04],\n",
       "         [ 1.1322e-01,  3.0090e-02, -5.6732e-02, -1.9446e-01, -1.3696e-01,\n",
       "           3.1177e-01, -2.4872e-02, -7.2754e-02, -1.6760e-01,  2.0654e-01],\n",
       "         [ 6.9702e-02,  1.4209e-01,  2.9404e-02,  2.1802e-01,  1.8811e-01,\n",
       "           3.3386e-02,  1.2378e-01, -2.3584e-01,  2.1130e-01,  1.4856e-01],\n",
       "         [ 1.6614e-01,  2.6123e-02,  1.7419e-01, -1.8530e-01,  2.8857e-01,\n",
       "           7.8491e-02, -2.6733e-01, -1.8677e-01, -2.0801e-01,  1.7407e-01],\n",
       "         [-2.7283e-02, -3.0298e-01,  3.0078e-01, -3.3722e-02, -3.7476e-02,\n",
       "          -5.5206e-02,  1.6089e-01,  1.6016e-01, -1.4099e-02,  8.0078e-02],\n",
       "         [-1.1981e-01, -6.1737e-02, -7.4890e-02, -2.8296e-01,  2.3474e-01,\n",
       "          -5.9570e-02, -1.4136e-01, -1.1707e-01,  3.3844e-02, -7.4043e-03],\n",
       "         [ 6.0913e-02,  1.3086e-01, -2.8748e-02,  2.6611e-01, -2.8882e-01,\n",
       "          -2.9077e-01, -2.8638e-01, -2.6367e-01,  9.3689e-02,  1.0846e-01],\n",
       "         [ 1.2683e-01, -6.2378e-02, -2.0398e-01,  2.9834e-01,  6.4453e-02,\n",
       "           8.6243e-02,  3.7415e-02,  2.8784e-01, -1.2703e-02,  4.3915e-02],\n",
       "         [-2.2974e-01, -1.7725e-01, -1.7395e-01, -1.9104e-01, -2.2632e-01,\n",
       "           1.4282e-01,  1.7444e-01,  6.6711e-02,  2.8564e-01,  1.4392e-01],\n",
       "         [-3.0225e-01, -5.5298e-02, -7.6111e-02, -6.4880e-02, -3.5973e-03,\n",
       "          -1.2344e-02, -1.4429e-01,  7.2693e-02,  1.5820e-01, -2.8125e-01],\n",
       "         [ 3.0853e-02,  1.5186e-01, -2.3560e-01,  7.7393e-02, -2.2498e-01,\n",
       "          -2.4268e-01, -7.1045e-02,  2.0154e-01,  1.3435e-02,  1.5967e-01],\n",
       "         [ 1.3403e-01,  6.7078e-02,  2.7466e-01,  1.4734e-01, -8.2214e-02,\n",
       "          -2.7148e-01,  2.5903e-01,  9.4421e-02, -1.9910e-01,  2.1765e-01],\n",
       "         [-6.9946e-02,  1.6528e-01, -2.4451e-01,  1.4648e-01,  1.4709e-01,\n",
       "           1.1196e-03,  5.2032e-02, -1.6174e-02, -4.5135e-02,  2.5244e-01],\n",
       "         [ 4.2480e-02,  1.2854e-01,  2.7856e-01,  1.0498e-01, -2.3914e-01,\n",
       "          -1.7786e-01,  1.3879e-01,  1.5427e-02, -3.0298e-01,  1.6467e-01]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([-0.2981, -0.1270, -0.2142, -0.0845, -0.1183, -0.1981,  0.0138,  0.0109,\n",
       "         -0.2356,  0.1774,  0.2527,  0.3120, -0.2346,  0.2241,  0.0157,  0.0607,\n",
       "         -0.2418,  0.1060,  0.0296,  0.1058, -0.1373,  0.1044, -0.0075, -0.1982,\n",
       "         -0.0453,  0.2876,  0.2817, -0.2317,  0.2098,  0.1152], device='cuda:0',\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([0.2933, 0.0328, 0.4102, 0.8385, 0.7705, 0.7688, 0.1496, 0.8441, 0.0049,\n",
       "         0.8729, 0.6137, 0.8909, 0.3187, 0.3351, 0.1173, 0.8980, 0.6859, 0.1560,\n",
       "         0.8973, 0.1757, 0.8740, 0.5098, 0.6870, 0.5428, 0.8056, 0.7795, 0.4494,\n",
       "         0.6531, 0.0140, 0.9588], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 1.2512e-01, -1.2286e-01, -7.8247e-02, -1.5723e-01,  1.2158e-01,\n",
       "           1.0559e-01, -8.5022e-02, -6.2744e-02, -1.8635e-03,  8.6426e-02,\n",
       "           1.2781e-01,  1.5186e-01,  3.9520e-02,  3.4241e-02, -5.4657e-02,\n",
       "          -1.7664e-01,  3.1815e-03,  1.1371e-01, -4.3579e-02, -1.5039e-01,\n",
       "           1.1133e-01,  2.2202e-02, -1.7871e-01, -1.9028e-02, -1.5186e-01,\n",
       "           3.4851e-02,  1.7502e-02,  1.0657e-01, -1.4136e-01,  1.6565e-01],\n",
       "         [-3.3051e-02, -1.4612e-01,  7.5928e-02,  3.6560e-02, -7.6477e-02,\n",
       "          -8.1116e-02,  6.5430e-02, -9.4360e-02,  4.7668e-02,  3.4851e-02,\n",
       "          -6.5796e-02,  5.0323e-02, -4.2267e-02, -1.1421e-02, -1.1658e-02,\n",
       "          -8.2397e-02, -1.5308e-01, -1.2341e-01,  8.8573e-05,  4.7760e-02,\n",
       "          -2.0645e-02, -8.8684e-02,  1.3367e-01,  3.4027e-02, -8.7280e-02,\n",
       "           1.0236e-01, -1.8237e-01,  9.6130e-02,  2.6321e-03,  3.6926e-02]],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([-0.0124, -0.1214], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_master_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_master_grads(test_model_params, test_master_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.5087e-03, -3.4847e-03,  1.6525e-02,  1.0452e-02, -6.8207e-03,\n",
       "          -6.7558e-03,  3.1815e-03,  6.4468e-04, -3.4027e-03, -1.1208e-02],\n",
       "         [ 2.7418e-05, -5.0604e-05,  2.1803e-04,  1.3256e-04, -1.0949e-04,\n",
       "          -9.2328e-05,  1.1981e-05,  6.6161e-06, -4.5061e-05, -1.9956e-04],\n",
       "         [-1.4248e-03,  4.6425e-03, -2.3865e-02, -1.4778e-02,  8.6975e-03,\n",
       "           9.9106e-03, -3.2921e-03,  8.3268e-05,  5.7373e-03,  1.4061e-02],\n",
       "         [ 5.3406e-03,  1.8433e-02, -4.1351e-02, -1.0513e-02,  1.6479e-02,\n",
       "           3.1219e-02,  3.1376e-04,  1.4580e-02,  7.6294e-03,  3.9276e-02],\n",
       "         [-1.9119e-02, -2.2087e-03,  5.4565e-02,  1.3596e-02, -1.4503e-02,\n",
       "          -3.0930e-02, -1.7456e-02, -2.6611e-02, -2.4216e-02, -2.9327e-02],\n",
       "         [ 7.5455e-03, -4.3335e-02,  5.6641e-02,  4.9896e-02, -5.0697e-03,\n",
       "          -4.0771e-02, -1.0414e-02, -1.3771e-02, -4.2496e-03, -1.4076e-02],\n",
       "         [-9.1028e-04,  2.0599e-03, -9.3765e-03, -1.5345e-03,  2.1305e-03,\n",
       "           1.1091e-03, -1.0228e-04,  1.8024e-03,  3.6383e-04,  4.0092e-03],\n",
       "         [ 2.4376e-03, -1.5945e-03,  8.3847e-03,  3.3550e-03, -4.4441e-03,\n",
       "          -8.4400e-04,  1.6184e-03,  1.6937e-03, -1.3008e-03, -8.4076e-03],\n",
       "         [-1.8239e-05,  2.7597e-05, -1.1730e-04, -8.1241e-05,  5.3704e-05,\n",
       "           4.6670e-05, -2.2829e-05, -1.2100e-05,  2.5570e-05,  8.8632e-05],\n",
       "         [ 7.2899e-03, -4.7073e-03,  1.9928e-02,  1.2077e-02, -8.2779e-03,\n",
       "           1.3828e-03,  4.4365e-03,  4.8866e-03,  7.5531e-04, -2.1774e-02],\n",
       "         [-8.0643e-03, -3.4857e-04,  2.4872e-02,  1.5762e-02, -1.3634e-02,\n",
       "          -3.0060e-02, -3.1605e-03, -6.8893e-03, -1.2749e-02, -1.0391e-02],\n",
       "         [ 5.8022e-03, -5.5504e-03,  3.9886e-02,  3.7720e-02, -1.4664e-02,\n",
       "          -1.7166e-02,  9.1324e-03,  5.5733e-03, -1.3496e-02, -2.1042e-02],\n",
       "         [ 1.1349e-03, -2.3556e-03,  1.2039e-02,  9.8114e-03, -4.7646e-03,\n",
       "          -5.2986e-03,  1.3723e-03,  1.5059e-03, -4.4861e-03, -8.6823e-03],\n",
       "         [-4.2844e-04, -1.2693e-03,  4.7569e-03,  2.1000e-03, -1.8921e-03,\n",
       "          -2.9354e-03, -3.3283e-04, -8.2731e-04, -1.3962e-03, -2.8477e-03],\n",
       "         [-2.7418e-05,  4.9257e-04, -2.1305e-03, -1.1024e-03,  8.1301e-04,\n",
       "           8.9502e-04, -4.3344e-04,  1.2124e-04,  3.3689e-04,  1.2951e-03],\n",
       "         [-1.9503e-03,  6.9923e-03, -3.2684e-02, -2.0569e-02,  1.2566e-02,\n",
       "           1.4175e-02, -4.6234e-03,  4.5300e-04,  7.1335e-03,  2.0416e-02],\n",
       "         [ 2.6875e-03, -6.9847e-03,  3.6407e-02,  2.1927e-02, -1.3542e-02,\n",
       "          -1.4221e-02,  4.6387e-03,  1.2201e-04, -8.3237e-03, -2.2186e-02],\n",
       "         [ 6.2256e-03, -8.4114e-04,  2.1698e-02,  3.2997e-03, -8.0109e-03,\n",
       "           2.7447e-03,  1.7729e-03,  2.1801e-03, -8.4152e-03, -9.5291e-03],\n",
       "         [-8.2111e-04,  3.2768e-03, -1.9867e-02, -1.3184e-02,  6.4163e-03,\n",
       "           7.7515e-03, -3.4485e-03,  8.4352e-04,  3.5248e-03,  9.8648e-03],\n",
       "         [-2.4748e-04,  2.4815e-03, -1.3412e-02, -6.5575e-03,  3.2101e-03,\n",
       "           5.1575e-03,  4.4060e-04,  1.2436e-03,  4.3640e-03,  5.8823e-03],\n",
       "         [-3.1662e-03,  9.6207e-03,  2.3315e-02,  2.5970e-02, -1.9974e-02,\n",
       "          -2.7115e-02, -1.2550e-02, -4.6501e-03, -1.6251e-02, -3.7720e-02],\n",
       "         [-5.0659e-03, -6.0959e-03,  1.8280e-02,  2.6951e-03, -4.2038e-03,\n",
       "          -1.3611e-02, -3.1528e-03, -8.1863e-03, -5.0392e-03, -1.2703e-02],\n",
       "         [-1.0681e-02,  1.6479e-02, -7.0068e-02, -3.2349e-02,  2.1332e-02,\n",
       "           1.1848e-02, -2.1225e-02, -6.9809e-03,  1.4977e-02,  4.9408e-02],\n",
       "         [-1.1759e-03,  1.9264e-03, -9.4910e-03, -7.1907e-03,  3.6716e-03,\n",
       "           3.9482e-03, -1.5945e-03, -9.0361e-04,  2.8095e-03,  5.8708e-03],\n",
       "         [ 3.8695e-04,  7.3318e-03, -1.9104e-02, -3.7975e-03,  1.1208e-02,\n",
       "           6.9389e-03, -1.1139e-02,  3.0117e-03, -5.3558e-03,  1.5007e-02],\n",
       "         [-1.3838e-03,  4.7188e-03, -2.4338e-02, -1.5144e-02,  8.9951e-03,\n",
       "           1.0193e-02, -3.1319e-03,  6.2048e-05,  5.9052e-03,  1.5015e-02],\n",
       "         [ 1.6003e-03, -4.1962e-03,  2.6840e-02,  2.3834e-02, -1.3550e-02,\n",
       "          -1.8402e-02,  2.3079e-03,  2.3460e-03, -1.0674e-02, -1.6373e-02],\n",
       "         [ 2.0981e-05, -4.3106e-04,  1.6575e-03,  1.9944e-04, -8.4591e-04,\n",
       "          -5.3740e-04, -2.6917e-04, -4.6802e-04,  1.5736e-05, -2.5597e-03],\n",
       "         [ 1.1832e-04,  3.7551e-06, -5.0497e-04, -8.4734e-04,  1.7977e-04,\n",
       "           6.7139e-04, -5.0962e-05, -1.7345e-05,  4.9114e-04,  2.2769e-05],\n",
       "         [ 4.0741e-03, -1.6739e-02,  3.0853e-02,  2.6836e-03, -1.4465e-02,\n",
       "          -9.3994e-03,  1.8473e-03, -8.9035e-03,  8.5983e-03, -5.7922e-02]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([-9.5367e-07,  2.3842e-07, -9.5367e-07,  0.0000e+00,  1.9073e-06,\n",
       "          0.0000e+00, -2.3842e-07,  2.3842e-07, -5.9605e-08, -9.5367e-07,\n",
       "          0.0000e+00,  1.9073e-06, -1.4305e-06,  2.3842e-07,  1.1921e-07,\n",
       "         -3.8147e-06, -1.9073e-06, -4.7684e-07, -9.5367e-07,  9.5367e-07,\n",
       "         -3.8147e-06, -2.3842e-06,  3.8147e-06,  4.7684e-07,  2.8610e-06,\n",
       "         -9.5367e-07,  0.0000e+00,  1.7881e-07,  0.0000e+00, -5.7220e-06],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 0.0031,  0.0012, -0.0005, -0.0207,  0.0297,  0.0374,  0.0189,  0.0031,\n",
       "         -0.0017,  0.0071,  0.0238,  0.0059,  0.0033,  0.0030,  0.0014, -0.0010,\n",
       "         -0.0010, -0.0436,  0.0014,  0.0116,  0.0199, -0.0141, -0.0207,  0.0013,\n",
       "          0.0102,  0.0002, -0.0101,  0.0016,  0.0224,  0.0239], device='cuda:0'),\n",
       " tensor([ 0.0121,  0.0018, -0.0118, -0.0148,  0.0152,  0.0143, -0.0115,  0.0024,\n",
       "         -0.0038,  0.0040,  0.0148,  0.0078,  0.0063,  0.0035, -0.0033, -0.0072,\n",
       "          0.0120,  0.0182, -0.0033, -0.0152,  0.0101,  0.0085, -0.0239, -0.0041,\n",
       "         -0.0049, -0.0052,  0.0153,  0.0008, -0.0110,  0.0099], device='cuda:0'),\n",
       " tensor([[ 5.7259e-03,  1.7071e-03,  1.3142e-03,  8.9661e-02,  1.1566e-01,\n",
       "           1.5405e-01, -1.8799e-02,  8.1787e-02,  1.6582e-04,  1.2042e-01,\n",
       "           7.5500e-02,  5.2032e-02,  1.2886e-02,  2.2003e-02, -3.7842e-03,\n",
       "           9.1171e-03, -4.6616e-03, -2.8671e-02, -2.7847e-02, -1.0254e-02,\n",
       "           1.3184e-01, -6.4697e-02,  4.5624e-02, -1.3641e-02, -1.2708e-01,\n",
       "          -2.7122e-03, -2.2598e-02,  1.0046e-01, -2.1839e-03,  1.7834e-01],\n",
       "         [-5.7411e-03, -1.7071e-03, -1.2922e-03, -8.9661e-02, -1.1566e-01,\n",
       "          -1.5405e-01,  1.8784e-02, -8.1787e-02, -1.6606e-04, -1.2042e-01,\n",
       "          -7.5500e-02, -5.2094e-02, -1.2909e-02, -2.1988e-02,  3.7842e-03,\n",
       "          -9.1248e-03,  4.6005e-03,  2.8671e-02,  2.7847e-02,  1.0246e-02,\n",
       "          -1.3196e-01,  6.4697e-02, -4.5624e-02,  1.3664e-02,  1.2708e-01,\n",
       "           2.7428e-03,  2.2614e-02, -1.0046e-01,  2.1858e-03, -1.7847e-01]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([ 0.0765, -0.0765], device='cuda:0', dtype=torch.float16)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.grad for param in test_model_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.5087e-03, -3.4847e-03,  1.6525e-02,  1.0452e-02, -6.8207e-03,\n",
       "          -6.7558e-03,  3.1815e-03,  6.4468e-04, -3.4027e-03, -1.1208e-02],\n",
       "         [ 2.7418e-05, -5.0604e-05,  2.1803e-04,  1.3256e-04, -1.0949e-04,\n",
       "          -9.2328e-05,  1.1981e-05,  6.6161e-06, -4.5061e-05, -1.9956e-04],\n",
       "         [-1.4248e-03,  4.6425e-03, -2.3865e-02, -1.4778e-02,  8.6975e-03,\n",
       "           9.9106e-03, -3.2921e-03,  8.3268e-05,  5.7373e-03,  1.4061e-02],\n",
       "         [ 5.3406e-03,  1.8433e-02, -4.1351e-02, -1.0513e-02,  1.6479e-02,\n",
       "           3.1219e-02,  3.1376e-04,  1.4580e-02,  7.6294e-03,  3.9276e-02],\n",
       "         [-1.9119e-02, -2.2087e-03,  5.4565e-02,  1.3596e-02, -1.4503e-02,\n",
       "          -3.0930e-02, -1.7456e-02, -2.6611e-02, -2.4216e-02, -2.9327e-02],\n",
       "         [ 7.5455e-03, -4.3335e-02,  5.6641e-02,  4.9896e-02, -5.0697e-03,\n",
       "          -4.0771e-02, -1.0414e-02, -1.3771e-02, -4.2496e-03, -1.4076e-02],\n",
       "         [-9.1028e-04,  2.0599e-03, -9.3765e-03, -1.5345e-03,  2.1305e-03,\n",
       "           1.1091e-03, -1.0228e-04,  1.8024e-03,  3.6383e-04,  4.0092e-03],\n",
       "         [ 2.4376e-03, -1.5945e-03,  8.3847e-03,  3.3550e-03, -4.4441e-03,\n",
       "          -8.4400e-04,  1.6184e-03,  1.6937e-03, -1.3008e-03, -8.4076e-03],\n",
       "         [-1.8239e-05,  2.7597e-05, -1.1730e-04, -8.1241e-05,  5.3704e-05,\n",
       "           4.6670e-05, -2.2829e-05, -1.2100e-05,  2.5570e-05,  8.8632e-05],\n",
       "         [ 7.2899e-03, -4.7073e-03,  1.9928e-02,  1.2077e-02, -8.2779e-03,\n",
       "           1.3828e-03,  4.4365e-03,  4.8866e-03,  7.5531e-04, -2.1774e-02],\n",
       "         [-8.0643e-03, -3.4857e-04,  2.4872e-02,  1.5762e-02, -1.3634e-02,\n",
       "          -3.0060e-02, -3.1605e-03, -6.8893e-03, -1.2749e-02, -1.0391e-02],\n",
       "         [ 5.8022e-03, -5.5504e-03,  3.9886e-02,  3.7720e-02, -1.4664e-02,\n",
       "          -1.7166e-02,  9.1324e-03,  5.5733e-03, -1.3496e-02, -2.1042e-02],\n",
       "         [ 1.1349e-03, -2.3556e-03,  1.2039e-02,  9.8114e-03, -4.7646e-03,\n",
       "          -5.2986e-03,  1.3723e-03,  1.5059e-03, -4.4861e-03, -8.6823e-03],\n",
       "         [-4.2844e-04, -1.2693e-03,  4.7569e-03,  2.1000e-03, -1.8921e-03,\n",
       "          -2.9354e-03, -3.3283e-04, -8.2731e-04, -1.3962e-03, -2.8477e-03],\n",
       "         [-2.7418e-05,  4.9257e-04, -2.1305e-03, -1.1024e-03,  8.1301e-04,\n",
       "           8.9502e-04, -4.3344e-04,  1.2124e-04,  3.3689e-04,  1.2951e-03],\n",
       "         [-1.9503e-03,  6.9923e-03, -3.2684e-02, -2.0569e-02,  1.2566e-02,\n",
       "           1.4175e-02, -4.6234e-03,  4.5300e-04,  7.1335e-03,  2.0416e-02],\n",
       "         [ 2.6875e-03, -6.9847e-03,  3.6407e-02,  2.1927e-02, -1.3542e-02,\n",
       "          -1.4221e-02,  4.6387e-03,  1.2201e-04, -8.3237e-03, -2.2186e-02],\n",
       "         [ 6.2256e-03, -8.4114e-04,  2.1698e-02,  3.2997e-03, -8.0109e-03,\n",
       "           2.7447e-03,  1.7729e-03,  2.1801e-03, -8.4152e-03, -9.5291e-03],\n",
       "         [-8.2111e-04,  3.2768e-03, -1.9867e-02, -1.3184e-02,  6.4163e-03,\n",
       "           7.7515e-03, -3.4485e-03,  8.4352e-04,  3.5248e-03,  9.8648e-03],\n",
       "         [-2.4748e-04,  2.4815e-03, -1.3412e-02, -6.5575e-03,  3.2101e-03,\n",
       "           5.1575e-03,  4.4060e-04,  1.2436e-03,  4.3640e-03,  5.8823e-03],\n",
       "         [-3.1662e-03,  9.6207e-03,  2.3315e-02,  2.5970e-02, -1.9974e-02,\n",
       "          -2.7115e-02, -1.2550e-02, -4.6501e-03, -1.6251e-02, -3.7720e-02],\n",
       "         [-5.0659e-03, -6.0959e-03,  1.8280e-02,  2.6951e-03, -4.2038e-03,\n",
       "          -1.3611e-02, -3.1528e-03, -8.1863e-03, -5.0392e-03, -1.2703e-02],\n",
       "         [-1.0681e-02,  1.6479e-02, -7.0068e-02, -3.2349e-02,  2.1332e-02,\n",
       "           1.1848e-02, -2.1225e-02, -6.9809e-03,  1.4977e-02,  4.9408e-02],\n",
       "         [-1.1759e-03,  1.9264e-03, -9.4910e-03, -7.1907e-03,  3.6716e-03,\n",
       "           3.9482e-03, -1.5945e-03, -9.0361e-04,  2.8095e-03,  5.8708e-03],\n",
       "         [ 3.8695e-04,  7.3318e-03, -1.9104e-02, -3.7975e-03,  1.1208e-02,\n",
       "           6.9389e-03, -1.1139e-02,  3.0117e-03, -5.3558e-03,  1.5007e-02],\n",
       "         [-1.3838e-03,  4.7188e-03, -2.4338e-02, -1.5144e-02,  8.9951e-03,\n",
       "           1.0193e-02, -3.1319e-03,  6.2048e-05,  5.9052e-03,  1.5015e-02],\n",
       "         [ 1.6003e-03, -4.1962e-03,  2.6840e-02,  2.3834e-02, -1.3550e-02,\n",
       "          -1.8402e-02,  2.3079e-03,  2.3460e-03, -1.0674e-02, -1.6373e-02],\n",
       "         [ 2.0981e-05, -4.3106e-04,  1.6575e-03,  1.9944e-04, -8.4591e-04,\n",
       "          -5.3740e-04, -2.6917e-04, -4.6802e-04,  1.5736e-05, -2.5597e-03],\n",
       "         [ 1.1832e-04,  3.7551e-06, -5.0497e-04, -8.4734e-04,  1.7977e-04,\n",
       "           6.7139e-04, -5.0962e-05, -1.7345e-05,  4.9114e-04,  2.2769e-05],\n",
       "         [ 4.0741e-03, -1.6739e-02,  3.0853e-02,  2.6836e-03, -1.4465e-02,\n",
       "          -9.3994e-03,  1.8473e-03, -8.9035e-03,  8.5983e-03, -5.7922e-02]],\n",
       "        device='cuda:0'),\n",
       " tensor([-9.5367e-07,  2.3842e-07, -9.5367e-07,  0.0000e+00,  1.9073e-06,\n",
       "          0.0000e+00, -2.3842e-07,  2.3842e-07, -5.9605e-08, -9.5367e-07,\n",
       "          0.0000e+00,  1.9073e-06, -1.4305e-06,  2.3842e-07,  1.1921e-07,\n",
       "         -3.8147e-06, -1.9073e-06, -4.7684e-07, -9.5367e-07,  9.5367e-07,\n",
       "         -3.8147e-06, -2.3842e-06,  3.8147e-06,  4.7684e-07,  2.8610e-06,\n",
       "         -9.5367e-07,  0.0000e+00,  1.7881e-07,  0.0000e+00, -5.7220e-06],\n",
       "        device='cuda:0'),\n",
       " tensor([ 0.0031,  0.0012, -0.0005, -0.0207,  0.0297,  0.0374,  0.0189,  0.0031,\n",
       "         -0.0017,  0.0071,  0.0238,  0.0059,  0.0033,  0.0030,  0.0014, -0.0010,\n",
       "         -0.0010, -0.0436,  0.0014,  0.0116,  0.0199, -0.0141, -0.0207,  0.0013,\n",
       "          0.0102,  0.0002, -0.0101,  0.0016,  0.0224,  0.0239], device='cuda:0'),\n",
       " tensor([ 0.0121,  0.0018, -0.0118, -0.0148,  0.0152,  0.0143, -0.0115,  0.0024,\n",
       "         -0.0038,  0.0040,  0.0148,  0.0078,  0.0063,  0.0035, -0.0033, -0.0072,\n",
       "          0.0120,  0.0182, -0.0033, -0.0152,  0.0101,  0.0085, -0.0239, -0.0041,\n",
       "         -0.0049, -0.0052,  0.0153,  0.0008, -0.0110,  0.0099], device='cuda:0'),\n",
       " tensor([[ 5.7259e-03,  1.7071e-03,  1.3142e-03,  8.9661e-02,  1.1566e-01,\n",
       "           1.5405e-01, -1.8799e-02,  8.1787e-02,  1.6582e-04,  1.2042e-01,\n",
       "           7.5500e-02,  5.2032e-02,  1.2886e-02,  2.2003e-02, -3.7842e-03,\n",
       "           9.1171e-03, -4.6616e-03, -2.8671e-02, -2.7847e-02, -1.0254e-02,\n",
       "           1.3184e-01, -6.4697e-02,  4.5624e-02, -1.3641e-02, -1.2708e-01,\n",
       "          -2.7122e-03, -2.2598e-02,  1.0046e-01, -2.1839e-03,  1.7834e-01],\n",
       "         [-5.7411e-03, -1.7071e-03, -1.2922e-03, -8.9661e-02, -1.1566e-01,\n",
       "          -1.5405e-01,  1.8784e-02, -8.1787e-02, -1.6606e-04, -1.2042e-01,\n",
       "          -7.5500e-02, -5.2094e-02, -1.2909e-02, -2.1988e-02,  3.7842e-03,\n",
       "          -9.1248e-03,  4.6005e-03,  2.8671e-02,  2.7847e-02,  1.0246e-02,\n",
       "          -1.3196e-01,  6.4697e-02, -4.5624e-02,  1.3664e-02,  1.2708e-01,\n",
       "           2.7428e-03,  2.2614e-02, -1.0046e-01,  2.1858e-03, -1.7847e-01]],\n",
       "        device='cuda:0'),\n",
       " tensor([ 0.0765, -0.0765], device='cuda:0')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.grad for param in test_master_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above comparison of model and master parameter gradients passes the eye test, but to be sure, let's double check that our FP32 gradients are all sufficiently close in value to their FP16 counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grads(param_list1, param_list2):\n",
    "    for param1, param2 in zip(param_list1, param_list2):\n",
    "        if param1.grad is None: assert param2.grad is None\n",
    "        else: assert torch.allclose(param1.grad.data.float(), param2.grad.data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grads(test_model_params, test_master_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in Apex to copy FP16 gradients to a FP32 master copy is `model_grads_to_master_grads`. Also, we'll of course, we need to support multiple parameter groups, so our final implementation will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_master_grads(model_param_groups, master_param_groups, flat_master:bool=False)->None:\n",
    "    for (model_param_group, master_param_group) in zip(model_param_groups, master_param_groups):\n",
    "        fp16.model_grads_to_master_grads(model_param_group, master_param_group, flat_master=flat_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper 4: Copying the FP32 master copy params back to FP16 model params\n",
    "Once the FP32 weight update calculation is complete, our \"master\" FP32 copy contains the updated model weights in FP32. In order to complete the next forward pass in FP16, we need to first copy these weights back to FP16 in the \"model\" copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._utils import _unflatten_dense_tensors\n",
    "\n",
    "def to_model_params(model_params, master_params, flat_master:bool=False)->None:\n",
    "    if flat_master:\n",
    "        for model_param, master_param in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):\n",
    "            model_param.data.copy_(master)\n",
    "    else:\n",
    "        for model_param, master_param in zip(model_params, master_params): model_param.data.copy_(master_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the weight update operation for all parameters in the FP32 \"master\" copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_master_params = [param + 0.1 * param.grad for param in test_master_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0602,  0.1168,  0.1621,  0.0817,  0.2056,  0.0634, -0.1922, -0.3105,\n",
       "          -0.2767,  0.1294],\n",
       "         [ 0.0450,  0.1744,  0.2864,  0.0373,  0.2698, -0.1588,  0.3079, -0.0688,\n",
       "          -0.1630,  0.2783],\n",
       "         [-0.2821,  0.0285, -0.1645,  0.2428,  0.1008,  0.2212, -0.2603,  0.2404,\n",
       "          -0.0626, -0.3084],\n",
       "         [-0.0293,  0.1952,  0.2235,  0.2919, -0.0848,  0.1047,  0.0517,  0.2576,\n",
       "          -0.0586,  0.1070],\n",
       "         [ 0.2428, -0.0627,  0.1372,  0.1090,  0.0320, -0.2865,  0.1726,  0.2986,\n",
       "           0.1580, -0.0902],\n",
       "         [-0.1125,  0.2474,  0.1685, -0.0744, -0.2971, -0.0801,  0.2260, -0.0043,\n",
       "          -0.3144, -0.2066],\n",
       "         [ 0.2887, -0.0840,  0.1117, -0.2980,  0.0969,  0.2670, -0.1924, -0.2747,\n",
       "           0.1255,  0.2404],\n",
       "         [ 0.0035,  0.0849,  0.1262,  0.2562,  0.2100, -0.3053,  0.0988, -0.3016,\n",
       "          -0.0748,  0.1003],\n",
       "         [-0.0463,  0.1359,  0.1727, -0.0281,  0.1628, -0.0461, -0.0049, -0.1990,\n",
       "          -0.1351,  0.0875],\n",
       "         [ 0.0953,  0.0992,  0.1314, -0.0590, -0.1660, -0.3067,  0.1640, -0.2698,\n",
       "          -0.3141,  0.0980],\n",
       "         [ 0.0722, -0.3096,  0.1696,  0.2716,  0.1034,  0.3068,  0.0411,  0.0045,\n",
       "          -0.0127, -0.2742],\n",
       "         [-0.1366, -0.1450,  0.1512, -0.2709, -0.1521, -0.0582, -0.1118, -0.0083,\n",
       "          -0.0070, -0.1115],\n",
       "         [ 0.2867,  0.0754,  0.1645, -0.1970, -0.0658, -0.1880,  0.1289, -0.2914,\n",
       "           0.0510,  0.1123],\n",
       "         [ 0.2509,  0.2211,  0.0515,  0.2993,  0.2569,  0.0287,  0.2868,  0.0516,\n",
       "           0.0474, -0.1036],\n",
       "         [-0.2649, -0.1826, -0.0006, -0.2252, -0.2165, -0.0086,  0.2225, -0.0111,\n",
       "           0.1479,  0.0506],\n",
       "         [-0.0499,  0.2910,  0.2250, -0.0776,  0.3015,  0.0328, -0.0358,  0.2840,\n",
       "          -0.2632,  0.0586],\n",
       "         [ 0.1975, -0.1005,  0.1554, -0.2561, -0.3097,  0.1737, -0.2478,  0.0710,\n",
       "           0.1953, -0.0025],\n",
       "         [ 0.1138,  0.0300, -0.0546, -0.1941, -0.1378,  0.3120, -0.0247, -0.0725,\n",
       "          -0.1684,  0.2056],\n",
       "         [ 0.0696,  0.1424,  0.0274,  0.2167,  0.1888,  0.0342,  0.1234, -0.2358,\n",
       "           0.2117,  0.1495],\n",
       "         [ 0.1661,  0.0264,  0.1729, -0.1860,  0.2889,  0.0790, -0.2673, -0.1866,\n",
       "          -0.2076,  0.1747],\n",
       "         [-0.0276, -0.3020,  0.3031, -0.0311, -0.0395, -0.0579,  0.1596,  0.1597,\n",
       "          -0.0157,  0.0763],\n",
       "         [-0.1203, -0.0623, -0.0731, -0.2827,  0.2343, -0.0609, -0.1417, -0.1179,\n",
       "           0.0333, -0.0087],\n",
       "         [ 0.0598,  0.1325, -0.0358,  0.2629, -0.2867, -0.2896, -0.2885, -0.2644,\n",
       "           0.0952,  0.1134],\n",
       "         [ 0.1267, -0.0622, -0.2049,  0.2976,  0.0648,  0.0866,  0.0373,  0.2878,\n",
       "          -0.0124,  0.0445],\n",
       "         [-0.2297, -0.1765, -0.1759, -0.1914, -0.2252,  0.1435,  0.1733,  0.0670,\n",
       "           0.2851,  0.1454],\n",
       "         [-0.3024, -0.0548, -0.0785, -0.0664, -0.0027, -0.0113, -0.1446,  0.0727,\n",
       "           0.1588, -0.2797],\n",
       "         [ 0.0310,  0.1514, -0.2329,  0.0798, -0.2263, -0.2445, -0.0708,  0.2018,\n",
       "           0.0124,  0.1580],\n",
       "         [ 0.1340,  0.0670,  0.2748,  0.1474, -0.0823, -0.2715,  0.2590,  0.0944,\n",
       "          -0.1991,  0.2174],\n",
       "         [-0.0699,  0.1653, -0.2446,  0.1464,  0.1471,  0.0012,  0.0520, -0.0162,\n",
       "          -0.0451,  0.2524],\n",
       "         [ 0.0429,  0.1269,  0.2816,  0.1052, -0.2406, -0.1788,  0.1390,  0.0145,\n",
       "          -0.3021,  0.1589]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([-0.2981, -0.1270, -0.2142, -0.0845, -0.1183, -0.1981,  0.0138,  0.0109,\n",
       "         -0.2356,  0.1774,  0.2527,  0.3120, -0.2346,  0.2241,  0.0157,  0.0607,\n",
       "         -0.2418,  0.1060,  0.0296,  0.1058, -0.1373,  0.1044, -0.0075, -0.1982,\n",
       "         -0.0453,  0.2876,  0.2817, -0.2317,  0.2098,  0.1152], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([0.2936, 0.0329, 0.4101, 0.8365, 0.7735, 0.7725, 0.1515, 0.8444, 0.0047,\n",
       "         0.8736, 0.6161, 0.8915, 0.3191, 0.3354, 0.1174, 0.8979, 0.6858, 0.1516,\n",
       "         0.8975, 0.1768, 0.8760, 0.5084, 0.6849, 0.5429, 0.8067, 0.7795, 0.4483,\n",
       "         0.6533, 0.0163, 0.9612], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([ 1.2112e-03,  1.7800e-04, -1.1808e-03, -1.4839e-03,  1.5154e-03,\n",
       "          1.4299e-03, -1.1517e-03,  2.4209e-04, -3.7923e-04,  3.9506e-04,\n",
       "          1.4828e-03,  7.7763e-04,  6.2637e-04,  3.4962e-04, -3.2916e-04,\n",
       "         -7.2107e-04,  1.1965e-03,  1.8154e-03, -3.3426e-04, -1.5171e-03,\n",
       "          1.0107e-03,  8.4906e-04, -2.3918e-03, -4.0612e-04, -4.9438e-04,\n",
       "         -5.1699e-04,  1.5303e-03,  7.9954e-05, -1.1028e-03,  9.8553e-04],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[ 0.1257, -0.1227, -0.0781, -0.1483,  0.1331,  0.1210, -0.0869, -0.0546,\n",
       "          -0.0018,  0.0985,  0.1354,  0.1571,  0.0408,  0.0364, -0.0550, -0.1757,\n",
       "           0.0027,  0.1108, -0.0464, -0.1514,  0.1245,  0.0157, -0.1741, -0.0204,\n",
       "          -0.1646,  0.0346,  0.0152,  0.1166, -0.1416,  0.1835],\n",
       "         [-0.0336, -0.1463,  0.0758,  0.0276, -0.0880, -0.0965,  0.0673, -0.1025,\n",
       "           0.0477,  0.0228, -0.0733,  0.0451, -0.0436, -0.0136, -0.0113, -0.0833,\n",
       "          -0.1526, -0.1205,  0.0029,  0.0488, -0.0338, -0.0822,  0.1291,  0.0354,\n",
       "          -0.0746,  0.1026, -0.1801,  0.0861,  0.0029,  0.0191]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([-0.0048, -0.1291], device='cuda:0', grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_master_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test out our implementation of `to_model_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_model_params(test_model_params, test_master_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0602,  0.1168,  0.1621,  0.0817,  0.2056,  0.0634, -0.1921, -0.3105,\n",
       "          -0.2766,  0.1294],\n",
       "         [ 0.0450,  0.1744,  0.2864,  0.0373,  0.2698, -0.1588,  0.3079, -0.0688,\n",
       "          -0.1630,  0.2783],\n",
       "         [-0.2822,  0.0285, -0.1646,  0.2428,  0.1008,  0.2212, -0.2603,  0.2404,\n",
       "          -0.0626, -0.3083],\n",
       "         [-0.0293,  0.1952,  0.2235,  0.2920, -0.0848,  0.1047,  0.0517,  0.2576,\n",
       "          -0.0586,  0.1069],\n",
       "         [ 0.2428, -0.0627,  0.1372,  0.1090,  0.0320, -0.2866,  0.1726,  0.2986,\n",
       "           0.1580, -0.0901],\n",
       "         [-0.1125,  0.2473,  0.1685, -0.0744, -0.2971, -0.0801,  0.2260, -0.0043,\n",
       "          -0.3145, -0.2067],\n",
       "         [ 0.2888, -0.0840,  0.1117, -0.2981,  0.0969,  0.2668, -0.1924, -0.2747,\n",
       "           0.1255,  0.2404],\n",
       "         [ 0.0035,  0.0849,  0.1262,  0.2561,  0.2100, -0.3052,  0.0988, -0.3015,\n",
       "          -0.0748,  0.1003],\n",
       "         [-0.0463,  0.1359,  0.1727, -0.0282,  0.1628, -0.0461, -0.0049, -0.1990,\n",
       "          -0.1351,  0.0875],\n",
       "         [ 0.0953,  0.0992,  0.1313, -0.0590, -0.1660, -0.3066,  0.1641, -0.2698,\n",
       "          -0.3142,  0.0980],\n",
       "         [ 0.0722, -0.3096,  0.1696,  0.2715,  0.1035,  0.3069,  0.0411,  0.0045,\n",
       "          -0.0127, -0.2742],\n",
       "         [-0.1366, -0.1450,  0.1512, -0.2710, -0.1521, -0.0582, -0.1118, -0.0083,\n",
       "          -0.0070, -0.1115],\n",
       "         [ 0.2866,  0.0754,  0.1646, -0.1970, -0.0659, -0.1880,  0.1289, -0.2913,\n",
       "           0.0510,  0.1123],\n",
       "         [ 0.2510,  0.2211,  0.0515,  0.2993,  0.2568,  0.0287,  0.2869,  0.0516,\n",
       "           0.0474, -0.1036],\n",
       "         [-0.2649, -0.1826, -0.0006, -0.2252, -0.2164, -0.0086,  0.2225, -0.0110,\n",
       "           0.1478,  0.0506],\n",
       "         [-0.0499,  0.2910,  0.2250, -0.0776,  0.3015,  0.0328, -0.0358,  0.2839,\n",
       "          -0.2632,  0.0586],\n",
       "         [ 0.1975, -0.1005,  0.1554, -0.2561, -0.3098,  0.1737, -0.2478,  0.0710,\n",
       "           0.1953, -0.0025],\n",
       "         [ 0.1138,  0.0300, -0.0546, -0.1941, -0.1378,  0.3120, -0.0247, -0.0725,\n",
       "          -0.1685,  0.2056],\n",
       "         [ 0.0696,  0.1425,  0.0274,  0.2167,  0.1887,  0.0341,  0.1234, -0.2357,\n",
       "           0.2117,  0.1495],\n",
       "         [ 0.1661,  0.0264,  0.1729, -0.1859,  0.2888,  0.0790, -0.2673, -0.1866,\n",
       "          -0.2075,  0.1747],\n",
       "         [-0.0276, -0.3020,  0.3032, -0.0311, -0.0395, -0.0579,  0.1597,  0.1597,\n",
       "          -0.0157,  0.0763],\n",
       "         [-0.1203, -0.0623, -0.0731, -0.2827,  0.2344, -0.0609, -0.1417, -0.1179,\n",
       "           0.0333, -0.0087],\n",
       "         [ 0.0598,  0.1326, -0.0358,  0.2629, -0.2866, -0.2896, -0.2886, -0.2644,\n",
       "           0.0952,  0.1134],\n",
       "         [ 0.1267, -0.0622, -0.2050,  0.2976,  0.0648,  0.0866,  0.0373,  0.2878,\n",
       "          -0.0124,  0.0445],\n",
       "         [-0.2297, -0.1765, -0.1759, -0.1914, -0.2252,  0.1436,  0.1733,  0.0670,\n",
       "           0.2852,  0.1454],\n",
       "         [-0.3025, -0.0548, -0.0786, -0.0664, -0.0027, -0.0113, -0.1447,  0.0727,\n",
       "           0.1588, -0.2798],\n",
       "         [ 0.0310,  0.1515, -0.2329,  0.0798, -0.2263, -0.2445, -0.0708,  0.2018,\n",
       "           0.0124,  0.1581],\n",
       "         [ 0.1340,  0.0670,  0.2749,  0.1473, -0.0823, -0.2715,  0.2590,  0.0944,\n",
       "          -0.1991,  0.2174],\n",
       "         [-0.0699,  0.1653, -0.2445,  0.1464,  0.1471,  0.0012,  0.0520, -0.0162,\n",
       "          -0.0451,  0.2524],\n",
       "         [ 0.0429,  0.1268,  0.2817,  0.1052, -0.2406, -0.1788,  0.1390,  0.0145,\n",
       "          -0.3020,  0.1589]], device='cuda:0', dtype=torch.float16,\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.2981, -0.1270, -0.2142, -0.0845, -0.1183, -0.1981,  0.0138,  0.0109,\n",
       "         -0.2356,  0.1774,  0.2527,  0.3120, -0.2346,  0.2241,  0.0157,  0.0607,\n",
       "         -0.2418,  0.1060,  0.0296,  0.1058, -0.1373,  0.1044, -0.0075, -0.1982,\n",
       "         -0.0453,  0.2876,  0.2817, -0.2317,  0.2098,  0.1152], device='cuda:0',\n",
       "        dtype=torch.float16, requires_grad=True), Parameter containing:\n",
       " tensor([0.2936, 0.0329, 0.4101, 0.8365, 0.7735, 0.7725, 0.1515, 0.8444, 0.0047,\n",
       "         0.8736, 0.6161, 0.8915, 0.3191, 0.3354, 0.1174, 0.8979, 0.6858, 0.1516,\n",
       "         0.8975, 0.1768, 0.8760, 0.5084, 0.6849, 0.5429, 0.8067, 0.7795, 0.4483,\n",
       "         0.6533, 0.0163, 0.9612], device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([ 1.2112e-03,  1.7800e-04, -1.1808e-03, -1.4839e-03,  1.5154e-03,\n",
       "          1.4299e-03, -1.1517e-03,  2.4209e-04, -3.7923e-04,  3.9506e-04,\n",
       "          1.4828e-03,  7.7763e-04,  6.2637e-04,  3.4962e-04, -3.2916e-04,\n",
       "         -7.2107e-04,  1.1965e-03,  1.8154e-03, -3.3426e-04, -1.5171e-03,\n",
       "          1.0107e-03,  8.4906e-04, -2.3918e-03, -4.0612e-04, -4.9438e-04,\n",
       "         -5.1699e-04,  1.5303e-03,  7.9954e-05, -1.1028e-03,  9.8553e-04],\n",
       "        device='cuda:0', requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.1257, -0.1227, -0.0781, -0.1483,  0.1332,  0.1210, -0.0869, -0.0546,\n",
       "          -0.0018,  0.0984,  0.1354,  0.1571,  0.0408,  0.0364, -0.0550, -0.1758,\n",
       "           0.0027,  0.1108, -0.0464, -0.1514,  0.1245,  0.0157, -0.1742, -0.0204,\n",
       "          -0.1646,  0.0346,  0.0152,  0.1166, -0.1416,  0.1835],\n",
       "         [-0.0336, -0.1462,  0.0758,  0.0276, -0.0880, -0.0965,  0.0673, -0.1025,\n",
       "           0.0476,  0.0228, -0.0734,  0.0451, -0.0435, -0.0136, -0.0113, -0.0833,\n",
       "          -0.1526, -0.1205,  0.0029,  0.0488, -0.0338, -0.0822,  0.1292,  0.0354,\n",
       "          -0.0746,  0.1026, -0.1801,  0.0861,  0.0029,  0.0191]],\n",
       "        device='cuda:0', dtype=torch.float16, requires_grad=True), Parameter containing:\n",
       " tensor([-0.0048, -0.1290], device='cuda:0', dtype=torch.float16,\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (param1, param2) in zip(test_model_params, test_master_params):\n",
    "    assert param1.requires_grad == param2.requires_grad\n",
    "    assert torch.allclose(param1.data.float(), param2.data.float(), atol=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that when copying FP16 params to FP32, the two lists matched with an absolute tolerance of 1r-8 (the PyTorch default for `torch.allclose`. However, when copying from FP32 back to FP16, the best we can do is get the two lists to match with an absolute tolerance of 1e-3.\n",
    "\n",
    "Also, we still this helper function to support parameter groups as well. We'll use Apex's equivaluent of `to_model_params`, `master_params_to_model_params`, to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_model_params(model_param_groups, master_param_groups, flat_master:bool=False)->None:\n",
    "    for (model_param_group, master_param_group) in zip(model_param_groups, master_param_groups):\n",
    "        fp16.master_params_to_model_params(model_param_group, master_param_group, flat_master=flat_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main mixed-precision callback\n",
    "We've finished our helper functions that transfer a model's layer weights to FP16, transfer weights back to FP32, transfer gradients to FP32, and transfer updated weights back to FP16. Now it's time to put all the pieces together and create a Callback class that will execute a mixed-precision training loop!\n",
    "\n",
    "That this class is so straightforward to implement is a testament to the flexibility of our training loop's (the `Learner` class) callback architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(Callback):\n",
    "    _order = 99\n",
    "    def __init__(self, loss_scale=512, flat_master=False):\n",
    "        assert torch.backends.cudnn.enabled, 'Mixed-precision training requires that the cuDNN be installed.'\n",
    "        self.loss_scale, self.flat_master = loss_scale, flat_master\n",
    "        \n",
    "    def begin_fit(self):\n",
    "        # Helper 1: Convert model (except for any batchnorm layers) to FP16:\n",
    "        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)\n",
    "        \n",
    "        # Helper 2: Creating a FP32 master copy of parameter weights\n",
    "        self.model_param_groups, self.master_param_groups = get_master(self.opt, self.flat_master)\n",
    "        # To place those FP32 master copy param groups inside the runner:\n",
    "        self.run.opt.param_groups = self.master_param_groups\n",
    "        \n",
    "    def after_fit(self): self.model.float()\n",
    "       \n",
    "    # Convert inputs to FP16 before forward pass\n",
    "    def begin_batch(self): self.run.xb = self.run.xb.half()\n",
    "        \n",
    "    # Convert preds to FP32 so that loss can be computed in FP32\n",
    "    def after_pred(self): self.run.pred = self.run.pred.float()\n",
    "        \n",
    "    # Scale loss to avoid gradient underflow (FP16 seeing non-zero grads as zero)\n",
    "    def after_loss(self): self.run.loss *= self.loss_scale\n",
    "        \n",
    "    def after_backward(self):\n",
    "        # Helper 3: Copy FP16 gradients (resulting from backprop) to FP32 master.\n",
    "        to_master_grads(self.model_param_groups, self.master_param_groups, self.flat_master)\n",
    "        \n",
    "        # Also unscale gradients so that weight update can be computed in FP32.\n",
    "        for param_group in self.master_param_groups:\n",
    "            for param in param_group:\n",
    "                if param.grad is not None: param.grad.div_(self.loss_scale)\n",
    "                    \n",
    "    def after_step(self):\n",
    "        # Be sure to zero out all gradients after the weight update step.\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Helper 4: Copy updated weights from FP32 master back to FP16 model, \n",
    "        #           so that next iteration's forward pass can be done in FP16.\n",
    "        to_model_params(self.model_param_groups, self.master_param_groups, self.flat_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this mixed-precision training callback out on Imagenette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr=3e-1\n",
    "n_outs = [64, 64, 128, 256] \n",
    "phases = combine_scheds([0.3, 0.7], cos_1cycle_anneal(min_lr, 3*min_lr, min_lr))\n",
    "scheduler = ParamScheduler('lr', phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll train without using mixed-precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_funcs = [partial(AvgStatsCallback, accuracy),\n",
    "                  CudaCallback,\n",
    "                  ProgressBarCallback,\n",
    "                  partial(BatchTransformXCallback, norm_imagenette),\n",
    "                  MixUp]\n",
    "learn = get_learner(n_outs, data, lr=min_lr, layer=conv_layer, callback_funcs=callback_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.936914</td>\n",
       "      <td>0.374205</td>\n",
       "      <td>1.844460</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.701108</td>\n",
       "      <td>0.502714</td>\n",
       "      <td>1.652163</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.525369</td>\n",
       "      <td>0.605165</td>\n",
       "      <td>1.089554</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.411922</td>\n",
       "      <td>0.661548</td>\n",
       "      <td>1.069016</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.307260</td>\n",
       "      <td>0.715371</td>\n",
       "      <td>0.906656</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.225333</td>\n",
       "      <td>0.760819</td>\n",
       "      <td>0.827631</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.136919</td>\n",
       "      <td>0.810377</td>\n",
       "      <td>0.789534</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.072786</td>\n",
       "      <td>0.852179</td>\n",
       "      <td>0.808262</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(8, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train with mixed-precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_funcs = [partial(AvgStatsCallback, accuracy),\n",
    "                  CudaCallback,\n",
    "                  ProgressBarCallback,\n",
    "                  partial(BatchTransformXCallback, norm_imagenette),\n",
    "                  MixUp,\n",
    "                  MixedPrecision]\n",
    "learn = get_learner(n_outs, data, lr=min_lr, layer=conv_layer, callback_funcs=callback_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.927650</td>\n",
       "      <td>0.377695</td>\n",
       "      <td>1.451271</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.691863</td>\n",
       "      <td>0.513107</td>\n",
       "      <td>1.486721</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.529405</td>\n",
       "      <td>0.599659</td>\n",
       "      <td>1.126288</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.406764</td>\n",
       "      <td>0.663254</td>\n",
       "      <td>0.975285</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.304548</td>\n",
       "      <td>0.714518</td>\n",
       "      <td>0.919934</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.207942</td>\n",
       "      <td>0.763378</td>\n",
       "      <td>0.823949</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.132919</td>\n",
       "      <td>0.810222</td>\n",
       "      <td>0.873254</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.071925</td>\n",
       "      <td>0.847448</td>\n",
       "      <td>0.814378</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(8, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that once training has completed, our model's weights are back in FP32 (`'torch.cuda.FloatTensor'`). Note that FP16 weights would have a [GPU tensor type](https://pytorch.org/docs/stable/tensors.html) of `'torch.cuda.HalfTensor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(next(learn.model.parameters()).type(), 'torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when I ran the mixed-precision training loop on my AWS p2.xlarge instance, there was no decrease in per-epoch training time. This is because the GPU  used in my p2.xlarge instance is too old and hasn't had any of the half-precision training optimizations built into it. If I had run my training loop using the newer NVIDIA Volta or 2080Ti GPUs, I would have seen a noticeable speed-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Loss Scaling\n",
    "We introduced loss scaling to overcome problem 2, which is when gradients computed during backpropagation that *should* be non-zero are interpreted as FP16 floats of value 0.0. In the above implemention of our mixed-precision training, we chose a loss scaling value of 512. However, is this ideal? Would a different value work better? Should it be less or greater than 512?\n",
    "\n",
    "The annoying thing here is that we now have yet another hyper-parameter to tune. Thankfully, there is a way we can avoid having to take the time to tune our loss scaling value! \n",
    "\n",
    "Conceptually, we want our scaling value to be as high as possible, so that there is as low as possible a chance that, when converted back to FP16, gradients that *shouldn't* be seen as zero are indeed not interpreted as zero. We want to go as high as we can, but stop just short of causing problem 3, which is where gradients overflow, or become NaN/infinity, when converted into FP16.\n",
    "\n",
    "Our approach will be to create yet another helper function that is able to detect when gradients overflow. We'll start our training by choosing a really large loss scaling value, and if our gradient overflow checker determines that gradients are overflowing, we'll decrease the loss scaling value by one half. Throughout training, our gradient overflow checker will continue to monitor things, and if gradients ever begin to overflow, loss scaling will again be decreased by half. We therefore refer to this entire mechanism as \"dynamic\" loss scaling!\n",
    "\n",
    "Note that we won't necessarily always want to halve our loss scaling. What about when training begins to converge and gradients naturally get smaller and smaller? In that case we'll want to make sure that our loss scaling is large enough to avoid gradient underflow. The Apex library's strategy for this is to double the loss scaling factor each time our model has gone a given number of iterations without the gradient overflow checking detecting any gradient overflow.\n",
    "\n",
    "So, to implement all this, we first need to develop a way to test if gradients have actually overflowed. This is pretty simple. An overflowed gradient is seen as just NaN. Although torch has a `torch.isnan` function that we could use to determine if any gradients are NaN, there's a faster way on the GPU: if we sum all the gradients together, and if at least one of the gradients is NaN, then their sum will also be NaN. We convert gradients back to FP32 before computing this sum.\n",
    "\n",
    "You might be wondering what, exactly, are the gradients that will be summed. The answer is that we will sum up the gradients for all parameters across each individual layer. In other words, at each iteration, we sum up gradients on a per-layer basis, when checking if any are NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_overflow(x):\n",
    "    grad_sum = float(x.float().sum())\n",
    "    return (grad_sum == float('inf') or grad_sum == float('-inf') or grad_sum != grad_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this logic performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512, 1024).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_overflow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set one of the \"gradients\" to infinity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[123, 145] = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify that using the sum trick indeed runs faster than using `torch.isnan` to see if *any* of the gradients are NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.1 µs ± 204 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_overflow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.8 µs ± 146 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.isnan(x).any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep! Summing all the gradients and then checking if the sum is NaN is faster! \n",
    "\n",
    "#### Helper 5: Checking param groups for gradient overflow\n",
    "We can now include the `test_overflow()` method inside a function, `grad_overflow()`, that, like the final versions of all our other helper functions support networks with parameter groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def grad_overflow(param_groups):\n",
    "    for param_group in param_groups:\n",
    "        for param in param_group:\n",
    "            if param.grad is not None:\n",
    "                test_overflow(param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main mixed-precision callback with dynamic loss scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixedPrecision(Callback):\n",
    "    _order = 99\n",
    "    def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24,\n",
    "                 div_factor=2., scale_wait=500):\n",
    "        assert torch.backends.cudnn.enabled, 'Mixed-precision training requires that the cuDNN be installed.'\n",
    "        self.flat_master = flat_master\n",
    "        self.dynamic = dynamic\n",
    "        self.max_loss_scale = max_loss_scale\n",
    "        self.div_factor = div_factor\n",
    "        self.scale_wait = scale_wait\n",
    "        self.loss_scale = max_loss_scale if dynamic else loss_scale\n",
    "        \n",
    "    def begin_fit(self):\n",
    "        # Helper 1: Convert model (except for any batchnorm layers) to FP16:\n",
    "        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)\n",
    "        \n",
    "        # Helper 2: Creating a FP32 master copy of parameter weights\n",
    "        self.model_param_groups, self.master_param_groups = get_master(self.opt, self.flat_master)\n",
    "        # To place those FP32 master copy param groups inside the runner:\n",
    "        self.run.opt.param_groups = self.master_param_groups\n",
    "        \n",
    "        # To count number of iterations without gradient overflow occurring.\n",
    "        if self.dynamic: self.count = 0\n",
    "        \n",
    "    def after_fit(self): self.model.float()\n",
    "       \n",
    "    # Convert inputs to FP16 before forward pass\n",
    "    def begin_batch(self): self.run.xb = self.run.xb.half()\n",
    "        \n",
    "    # Convert preds to FP32 so that loss can be computed in FP32\n",
    "    def after_pred(self): self.run.pred = self.run.pred.float()\n",
    "        \n",
    "    # Scale loss to avoid gradient underflow (FP16 seeing non-zero grads as zero)\n",
    "    def after_loss(self): self.run.loss *= self.loss_scale\n",
    "        \n",
    "    def after_backward(self):\n",
    "        # Helper 5: check whether gradient overflow is occurring.\n",
    "        if self.dynamic and grad_overflow(self.model_param_groups):\n",
    "            # Divide loss scale factor by the div_factor (usually 2.)\n",
    "            # if there is gradient overflow.\n",
    "            self.loss_scale /= self.div_factor\n",
    "            # We just zero out the gradients and skip the weight \n",
    "            # update step if there are NaN gradients.\n",
    "            self.model.zero_grad()\n",
    "            return True\n",
    "        \n",
    "        # Helper 3: Copy FP16 gradients (resulting from backprop) to FP32 master.\n",
    "        to_master_grads(self.model_param_groups, self.master_param_groups, self.flat_master)\n",
    "        \n",
    "        # Also unscale gradients so that weight update can be computed in FP32.\n",
    "        for param_group in self.master_param_groups:\n",
    "            for param in param_group:\n",
    "                if param.grad is not None: param.grad.div_(self.loss_scale)\n",
    "      \n",
    "        # Check if we can double the loss scale factor (if it's been \n",
    "        # long enough since a gradient overflow occurred).\n",
    "        if self.dynamic:\n",
    "            self.count += 1\n",
    "            if self.count == self.scale_wait:\n",
    "                self.count = 0\n",
    "                self.loss_scale *= self.div_factor\n",
    "    \n",
    "    def after_step(self):\n",
    "        # Be sure to zero out all gradients after the weight update step.\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Helper 4: Copy updated weights from FP32 master back to FP16 model, \n",
    "        #           so that next iteration's forward pass can be done in FP16.\n",
    "        to_model_params(self.model_param_groups, self.master_param_groups, self.flat_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lower max_loss_scale than the above MixedPrecision class'\n",
    "# default value. Because my one cycle model uses a higher than \n",
    "# typical minimum learning rate (0.3).\n",
    "max_loss_scale = 2.**12\n",
    "callback_funcs = [partial(AvgStatsCallback, accuracy),\n",
    "                  CudaCallback,\n",
    "                  ProgressBarCallback,\n",
    "                  partial(BatchTransformXCallback, norm_imagenette),\n",
    "                  MixUp,\n",
    "                  partial(MixedPrecision, max_loss_scale=max_loss_scale)]\n",
    "learn = get_learner(n_outs, data, lr=min_lr, layer=conv_layer, callback_funcs=callback_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>code_show_err=false;</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prevent next cell from displaying pink error box\n",
    "# re. assertion errors thrown by dataloader.py that are ignored.\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>code_show_err=false;</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.942185</td>\n",
       "      <td>0.372732</td>\n",
       "      <td>1.846166</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.716588</td>\n",
       "      <td>0.502714</td>\n",
       "      <td>1.511381</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.549313</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>1.111166</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.406133</td>\n",
       "      <td>0.661470</td>\n",
       "      <td>1.151800</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.318605</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.960298</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.234019</td>\n",
       "      <td>0.752753</td>\n",
       "      <td>0.917452</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.157992</td>\n",
       "      <td>0.797115</td>\n",
       "      <td>0.804901</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.092179</td>\n",
       "      <td>0.836668</td>\n",
       "      <td>0.846601</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(8, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss scaler that our dynamic loss scaling logic settled upon after 8 epochs of training is *much* larger than the value, 512, that we used by default in our earlier implementation of mixed precision training that didn't include dynamic loss scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callbacks[-1].loss_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "{\n",
       "const ip = IPython.notebook\n",
       "if (ip) {\n",
       "    ip.save_notebook()\n",
       "    console.log('a')\n",
       "    const s = `!python notebook2script_my_reimplementation.py ${ip.notebook_name}`\n",
       "    if (ip.kernel) { ip.kernel.execute(s) }\n",
       "}\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_auto_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
